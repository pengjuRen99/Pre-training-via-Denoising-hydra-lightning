_target_: src.models.module.LNNP   # LightningModule

extras:
  lr_warmup_steps: 10000        # How many steps to warm-up over. Defaults to 0 for no warm-up
  ema_alpha_y: 1.0              # The amount of influence of new losses on the exponential moving average of y
  ema_aplha_dy: 1.0             # The amount of influence of new losses on the exponential moving average of dy
  energy_weight: 1.0            # Weighting factor for energies in the loss function
  force_weight: 1.0             # Weighting factor for forces in the loss function
  denoising_weight: 0.1         # Weighting factor for denoising in the loss function.
  contrastive_weight: 0.0       # 加入对比和重构损失
  reconstruct_weight: 0.0

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0004                               
  weight_decay: 0.0

# scheduler:    # choices=['cosine', 'reduce_on_plateau']
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: min
#   factor: 0.8          # Minimum learning rate before early stop
#   patience: 15         # Patience for lr-schedule. Patience per eval-interval of validation
#   min_lr: 1e-7        # Minimum learning rate before early stopping

# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   _partial_: true
#   T_max: 400000       # Cosine length if lr_schedule is cosine.

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  _partial_: true
  T_0: 100000       # Cosine length if lr_schedule is cosine.
  T_mult: 2
  eta_min: 1e-7

net:
  # _target_: src.models.components.simple_dense_net.SimpleDenseNet   # equivariant-transformer
  # architectural args
  model: equivariant-transformer          # choices=["graph-network", "transformer", "equivariant-transformer"]
  embedding_dimension: 256                # Embedding dimension
  num_layers: 8                           # Number of interaction layers in the model
  num_rbf: 64                             # Number of radial basis functions in model
  activation: silu                        # Activation function, choices=list(act_class_mapping.keys())
  rbf_type: expnorm                       # Type of distance expansion, choices=list(rbf_class_mapping.keys())
  trainable_rbf: false                    # If distance expansion functions should be trainable
  neighbor_embedding: true                # If a neighbor embedding should be applied before interactions
  aggr: add                               # Aggregation function, choices=['add', 'mean', 'max']
  # Transformer specific
  distance_influence: both                # Where distance information is included inside the attention, choices=['keys', 'values', 'both', 'none']
  attn_activation: silu                   # Attention activation function, choices=list(act_class_mapping.keys())
  num_heads: 8                            # Number of attention heads
  layernorm_on_vec: whitened                  # Whether to apply an equivariant layer norm to vec features. Off by default. choices=['whitened']
  # other args
  derivative: false                       # If true, take the derivative of the prediction w.r.t coordinates
  cutoff_lower: 0.0                       # Lower cutoff for interatomic distances  
  cutoff_upper: 5.0                       # Upper cutoff for interatomic distances
  atom_filter: -1                         # Only sum over atoms with Z > atom_filter
  max_z: 100                              # Maximum atomic number that fits in the embedding matrix
  max_num_neighbors: 32                   # Maximum number of neighbors to consider in the network
  # standardize: false                      # If true, multiply prediction by dataset std and add mean
  reduce_op: add                          # Reduce operation to apply to atomic predictions, choices=['add', 'mean'] 
  position_noise_scale: 0.005              # Scale of Gaussian noise added to positions. 

  load_model: null                     # Path to model checkpoint to load before training
  pretrained_model: experiments/pretrain-MultiSpec-con_recon-SpecFormer-log10-no_lr_reduce-offical/last.ckpt                      # If true, load a pretrained model from torchmd-net

  prior_model: null       # Which prior model to use      et. Atomref
  # _target_: src.models.components.priors.Atomref

  output_model: Scalar      # The type of output model      Scalar

  output_model_noise: VectorOutput    # The type of output model for denoising    VectorOutput
    # _target_: null
  reduce_lr_when_bad: false
  use_dataset_md17: false
  lr_factor: 0.8
  lr_patience: 15

  uv_model: SpecFormer # SpecFormer
  input_data_norm_type: log10
  output_model_spec: null
  output_model_mol: null
  # SpecFormer [uv, ir, raman]
  patch_len: [20,50,50]
  stride: [10,25,25]
  mask_ratios: [0.1,0.1,0.1]


# compile model for faster training with pytorch 2.0
compile: false
